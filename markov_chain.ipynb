{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnVYZnYbkATN"
      },
      "source": [
        "# Markov Chain\n",
        "\n",
        "In this exercise, you will work with Markov Chains to generate text based on probabilistic patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwVhrAfLf7_x"
      },
      "source": [
        "## Question 10 (15 points):\n",
        "\n",
        "1.   **Load and Tokenize:** Begin by loading the `Brown` corpus from the `NLTK` library and tokenize it into words using `word_tokenize`. The `Brown` corpus will serve as our source text.\n",
        "\n",
        "2.   **Build N-gram Models:** Create various n-gram models (e.g., unigrams, bigrams) to capture word sequences from the tokenized text. While building these models, calculate transition probabilities between n-grams.\n",
        "\n",
        "3.   **Generate Text:** Implement a text generation function that uses the calculated probabilities. This function should generate text based on the patterns observed in the corpus.\n",
        "\n",
        "4.   **Print Results:** For each n-gram order (1, 2, 3, and 4-gram), print the generated text. You can inspect the results to understand how different n-gram orders affect the generated text.\n",
        "\n",
        "**Example of Transition Probabilities:**\n",
        "\n",
        "For a 2-gram, the probabilities might look like this:\n",
        "\n",
        "```\n",
        "{('to', 'wait'): {'one': 0.043478260869565216,\n",
        "  'until': 0.21739130434782608,\n",
        "  ',': 0.21739130434782608,\n",
        "  'for': 0.17391304347826086,\n",
        "  'a': 0.043478260869565216,\n",
        "  'his': 0.043478260869565216,\n",
        "  '.': 0.08695652173913043,\n",
        "  'to': 0.043478260869565216,\n",
        "  'till': 0.08695652173913043,\n",
        "  'before': 0.043478260869565216},...}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ERdn7xKrkINu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to\n",
            "[nltk_data]     C:\\Users\\haesh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\haesh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import brown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_ld_ol8Gwpe0"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "from nltk import ngrams\n",
        "from nltk.corpus import brown\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yvjMUqzaDGh8"
      },
      "outputs": [],
      "source": [
        "# Set the random seed for 42\n",
        "SEED = 42\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0upZNX0oDIE0"
      },
      "outputs": [],
      "source": [
        "# Read the text corpus\n",
        "corpus = brown.words()\n",
        "# str_corpus = ' '.join(corpus)\n",
        "tokens = word_tokenize(' '.join(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ghRyIGIJDLDm"
      },
      "outputs": [],
      "source": [
        "def build_ngram_model(tokens, n):\n",
        "    \"\"\"\n",
        "    Build an n-gram model from a list of tokens.\n",
        "\n",
        "    Args:\n",
        "    - tokens (list): List of tokens from the corpus.\n",
        "    - n (int): The order of the n-grams to build (e.g., 1 for unigrams, 2 for bigrams, 3 for trigrams).\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary containing n-grams as keys and their associated probability distributions as values.\n",
        "    \"\"\"\n",
        "    ngram_model = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    # Generate N-grams\n",
        "    ngrams_list = [tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)] # to remove puncts add: if all(char not in string.punctuation for char in tokens[i:i + n])\n",
        "    # print(ngrams_list)\n",
        "\n",
        "    # Generate N+1-grams\n",
        "    ngramsP1_list = [tuple(tokens[i:i + n + 1]) for i in range(len(tokens) - n)]\n",
        "\n",
        "    # Count occurrences of N-grams\n",
        "    ngram_counts = nltk.FreqDist(ngrams_list)\n",
        "\n",
        "    # Count occurrences of N+1-grams\n",
        "    ngramP1_counts = nltk.FreqDist(ngramsP1_list)\n",
        "\n",
        "    # Calculate transition probabilities\n",
        "    for ngram, count in ngramP1_counts.items():\n",
        "        context = tuple(ngram[:-1])\n",
        "        word = ngram[-1]\n",
        "        probability = count / ngram_counts[context]\n",
        "        ngram_model[context][word] = probability\n",
        "\n",
        "    return ngram_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3PzxWRSMDQnv"
      },
      "outputs": [],
      "source": [
        "def generate_text(ngrams, length):\n",
        "    \"\"\"\n",
        "    Generate text using an n-gram model.\n",
        "\n",
        "    Args:\n",
        "    - ngrams (dict): An n-gram model dictionary generated by build_ngram_model.\n",
        "    - length (int): The desired length of the generated text in tokens.\n",
        "\n",
        "    Returns:\n",
        "    - str: The generated text as a string.\n",
        "    \"\"\"\n",
        "     # Check if the n-gram model is empty\n",
        "    if not ngrams:\n",
        "        raise ValueError(\"The n-gram model is empty.\")\n",
        "\n",
        "    # Get the order of the n-gram model\n",
        "    n = len(next(iter(ngrams.keys())))\n",
        "\n",
        "    # Select a random starting context from the n-gram model\n",
        "    current_context = random.choice(list(ngrams.keys()))\n",
        "\n",
        "    # Initialize the generated text with the starting context\n",
        "    generated_text = list(current_context)\n",
        "\n",
        "    # Generate text until the desired length is reached\n",
        "    while len(generated_text) < length:\n",
        "        # Check if the current context is present in the n-gram model\n",
        "        if current_context in ngrams:\n",
        "            # Get the next word based on the current context\n",
        "            next_word = random.choices(\n",
        "                list(ngrams[current_context].keys()),\n",
        "                weights=list(ngrams[current_context].values())\n",
        "            )[0]\n",
        "\n",
        "            # Update the current context for the next iteration\n",
        "            current_context = tuple(list(current_context[1:]) + [next_word])\n",
        "\n",
        "            # Append the next word to the generated text\n",
        "            generated_text.append(next_word)\n",
        "        else:\n",
        "            # If the current context is not present, select a new random context\n",
        "            current_context = random.choice(list(ngrams.keys()))\n",
        "\n",
        "    # Convert the generated text list to a string\n",
        "    generated_text_str = ' '.join(generated_text)\n",
        "\n",
        "    return generated_text_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "y-XTBnyJDTHv"
      },
      "outputs": [],
      "source": [
        "# Build n-gram models of different orders with probabilities\n",
        "n1_gram = build_ngram_model(tokens, n=1)\n",
        "n2_gram = build_ngram_model(tokens, n=2)\n",
        "n3_gram = build_ngram_model(tokens, n=3)\n",
        "n4_gram = build_ngram_model(tokens, n=4)\n",
        "n5_gram = build_ngram_model(tokens, n=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lmE_hfabDVPT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1-Gram Generated Text: 2a ( whom I was no idea . If you worry about this moment she returned to their voices beg your feet , who had . The 15th anniversary of the first half of phosphate rock `` ! Yet it , the papers in bins . 31 , of stenography\n",
            "2-Gram Generated Text: and applications to leave a party , governments or races or institutions in the notes within Fig . 7 . If they do relax some of his movements was tentative . The Royal Air Force of 91 combat wings and even world attention is the airfield . Eliminate the vulnerability\n",
            "3-Gram Generated Text: and a pulmonary vein on the other hand , if he chooses to be . Your chauffeur 's expenses will average between $ 150 and $ 200 for the Custom version ) . This tied in closely with the principal property disposal installations of the Federal Constitution was that the\n",
            "4-Gram Generated Text: a.m. to visit the Kyoto University where Mr. Washizu is attending . I was amazed at the way he became more and more Fiorello as the evening progressed , until one had to catch one 's self up and remember that this was n't really LaGuardia come back among us\n",
            "5-Gram Generated Text: unmistakable symptoms of leprosy . Hell 's own amount of chaulmoogra oil did nothing to alleviate their torment ; ; they expired amid indescribable fantods , imploring the Blessed One to forgive their desecration . Any reputable French interne can supply you with a dozen similar instances , and I\n"
          ]
        }
      ],
      "source": [
        "# Generate text using the n-gram models with transition probabilities\n",
        "length = 50\n",
        "n1_generated_text = generate_text(n1_gram, length)\n",
        "n2_generated_text = generate_text(n2_gram, length)\n",
        "n3_generated_text = generate_text(n3_gram, length)\n",
        "n4_generated_text = generate_text(n4_gram, length)\n",
        "n5_generated_text = generate_text(n5_gram, length)\n",
        "\n",
        "# Print the generated text\n",
        "print(f'1-Gram Generated Text: {n1_generated_text}')\n",
        "print(f'2-Gram Generated Text: {n2_generated_text}')\n",
        "print(f'3-Gram Generated Text: {n3_generated_text}')\n",
        "print(f'4-Gram Generated Text: {n4_generated_text}')\n",
        "print(f'5-Gram Generated Text: {n5_generated_text}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
